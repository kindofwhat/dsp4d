{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3APxJ1HOzma",
        "outputId": "e7453edb-fad4-41a0-a13c-e0d45e3669e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the official OpenAI library\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Mdp0e7PA8t",
        "outputId": "17405885-9820-4770-d5be-03ed225113d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending request to: https://ai.bniweb.ch using model: tinydolphin:latest\n",
            "\n",
            "--- Response ---\n",
            "Model: tinydolphin:latest\n",
            "Content: LLMs (Linearized Linear Mirrors) are also known as Large-mirks or Large Magnetic Mirrors. This name is due to their size and the large surface area they have for generating large magnetic fields, which can be used for applications such as levitation, deflecting radiation, creating powerful magnetic fields, and more.\n",
            "Total Tokens Used: 119\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1. Open WebUI URL. Use the full protocol (https://).\n",
        "# Open WebUI often uses a /api or /api/v1 prefix, but the modern openai library\n",
        "# handles the /v1/chat/completions part automatically.\n",
        "BASE_URL = \"https://ai.bniweb.ch\"\n",
        "\n",
        "# 2. Your API Token\n",
        "# WARNING: In a production environment, never hardcode this.\n",
        "# Use Colab Secrets or environment variables for security.\n",
        "API_TOKEN = \"123456\"\n",
        "\n",
        "# 3. Model Name\n",
        "# Replace 'llama3' with the actual model ID you want to use from your Ollama server.\n",
        "# You can find the available models in your Open WebUI settings.\n",
        "MODEL_NAME = \"tinydolphin:latest\"\n",
        "\n",
        "# --- Custom Header: Try to mimic a browser or a more generic tool ---\n",
        "CUSTOM_HEADERS = {\n",
        "    # This User-Agent is less likely to be flagged as a generic script\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\",\n",
        "    \"Authorization\": f\"Bearer {API_TOKEN}\" # Authentication is still needed\n",
        "}\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=API_TOKEN,\n",
        "    base_url=f\"{BASE_URL}/api\",\n",
        "    # Pass the custom headers to the underlying HTTP client\n",
        "    default_headers=CUSTOM_HEADERS\n",
        ")\n",
        "\n",
        "# --- Perform Chat Completion ---\n",
        "try:\n",
        "    print(f\"Sending request to: {BASE_URL} using model: {MODEL_NAME}\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain in one short sentence why LLMs are called 'large'.\"}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=50\n",
        "    )\n",
        "\n",
        "    # --- Print Result ---\n",
        "    print(\"\\n--- Response ---\")\n",
        "    print(\"Model:\", response.model)\n",
        "    print(\"Content:\", response.choices[0].message.content.strip())\n",
        "    print(\"Total Tokens Used:\", response.usage.total_tokens)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ERROR ---\")\n",
        "    print(f\"An error occurred. Check your URL, API Token, Model Name, and Open WebUI API settings.\")\n",
        "    print(f\"Error details: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
