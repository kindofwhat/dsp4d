# Critical evaluation report (working notes)

## Scope
This report critically evaluates claims made in:

- `paper/chapters/02-theory/index.md`
- `paper/chapters/03-methodology/index.md`

The goal is not to rewrite the thesis text yet, but to assess whether key claims are (a) correctly scoped and (b) supported by the cited literature and the locally stored reference PDFs in `paper/references/`.

## High-level conclusions

### 1) Core claims are directionally correct, but the strongest wording is vulnerable
Several key statements (especially around automated evaluation and bias) are aligned with current research, but some phrasing is stronger than what the cited works (at least their abstracts) directly justify.

### 2) The citation trail is currently not auditable end-to-end
Multiple cited items are missing from `paper/references/`, and some local files are duplicated or incorrect. This undermines the reliability of the reference trail even when the narrative is correct.

### 3) Methodology is coherent but will need explicit bias mitigations
The methodology plans to use an *LLM-as-a-judge* component while the theory chapter notes judge biases. This is not inconsistent, but it implies the final evaluation must include explicit controls (e.g., length control, multiple judges, calibration, or targeted human spot-checks).

## Claim checks (selected)

### Bias in LLM-as-a-Judge

**Self-preference bias** (`@panickssery2024llmevaluatorsrecognizefavor`)

- Supported directionally: the cited work (arXiv:2404.13076) explicitly studies self-preference and links it to self-recognition.
- Risk of overreach in phrasing: “favor outputs generated by themselves *or similar architectures*” goes beyond “self” unless the paper explicitly demonstrates the “similar architectures” extension.

**Length / verbosity bias** (`@saito2024lengthbias`)

- Supported directionally: the cited work (arXiv:2310.10076) studies verbosity bias in preference labeling and reports settings where GPT-4 prefers longer answers more than humans.
- Risk of overreach in phrasing: “prefer longer responses *regardless of quality*” is stronger than “prefer longer when qualities are similar / as a confounder”. The claim should be scoped to the problem setting and framed as a confounder rather than a total override of quality.

### Weakness of traditional metrics (BLEU/ROUGE)

- Directionally correct and widely supported in the literature.
- Local verification is currently blocked by missing PDFs for some cited sources.

### Data contamination

- Directionally correct: if benchmarks enter training corpora, evaluation becomes inflated.
- Risk of overreach: statements about prevalence trends (“increasingly prevalent”) should ideally be backed by measurements across models/time or toned down.

### Medical privacy/security

- Memorization/extraction risk (`@carlini2021extracting`) is well-supported.
- Adversarial/jailbreak risk (`@zou2023universal`) is relevant, though the term “prompt injection” may be slightly mismatched depending on the intended meaning (alignment bypass vs. RAG context exfiltration).

## Reference integrity findings

### Missing locally stored PDFs
Multiple citations used in `02-theory` do not have corresponding files in `paper/references/` (e.g., the two LLM judge bias papers, plus several older foundational references and several placeholder stubs).

### Incorrect / duplicated local reference files (must fix)
- At least one cited PDF filename is mapped to the wrong paper content (example: `wu2025dualstage.pdf`).
- Some PDFs are duplicated byte-for-byte under different filenames, implying earlier download/mapping errors.

## Recommended next actions

1. Repair `paper/references.bib` entries whose metadata/arXiv IDs are wrong or placeholders.
2. Re-download PDFs after correcting BibTeX so that citation keys map to the intended papers.
3. Quarantine or remove incorrect/duplicate PDFs from `paper/references/` before re-downloading.
4. Re-run a “missing references” check to produce a final list of remaining missing/unavailable sources.
